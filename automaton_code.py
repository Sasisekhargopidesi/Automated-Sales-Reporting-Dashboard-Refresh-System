# -*- coding: utf-8 -*-
"""Automaton_code.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1JjvvOpqBWo2pNE7BAyT070e7eL2IXP25

Power BI automation — trigger dataset refresh (Python)

File: scripts/powerbi_refresh.py
Purpose: Use Azure AD app credentials to get an access token and call the Power BI REST API to trigger a dataset refresh (works for service principal or app-only auth).
"""

#!/usr/bin/env python3
"""
powerbi_refresh.py
Trigger a Power BI dataset refresh using the Power BI REST API + Azure AD (MSAL).

Required env vars:
- TENANT_ID
- CLIENT_ID
- CLIENT_SECRET
- WORKSPACE_ID (Power BI group id)
- DATASET_ID

Usage:
    python powerbi_refresh.py
"""

import os
import requests
import time
from msal import ConfidentialClientApplication

TENANT_ID = os.getenv("TENANT_ID")
CLIENT_ID = os.getenv("CLIENT_ID")
CLIENT_SECRET = os.getenv("CLIENT_SECRET")
WORKSPACE_ID = os.getenv("WORKSPACE_ID")  # aka groupId
DATASET_ID = os.getenv("DATASET_ID")

if not all([TENANT_ID, CLIENT_ID, CLIENT_SECRET, WORKSPACE_ID, DATASET_ID]):
    raise SystemExit("Missing required environment variables. See script header.")

AUTHORITY = f"https://login.microsoftonline.com/{TENANT_ID}"
SCOPE = ["https://analysis.windows.net/powerbi/api/.default"]
POWERBI_API_BASE = "https://api.powerbi.com/v1.0/myorg"


def get_access_token():
    """
    Acquire an Azure AD access token for the Power BI API using MSAL ConfidentialClientApplication.
    """
    app = ConfidentialClientApplication(
        client_id=CLIENT_ID,
        client_credential=CLIENT_SECRET,
        authority=AUTHORITY,
    )
    result = app.acquire_token_for_client(scopes=SCOPE)
    if "access_token" in result:
        return result["access_token"]
    raise Exception(f"Failed to get access token: {result}")


def trigger_refresh(access_token):
    """
    Trigger a dataset refresh for the configured workspace and dataset.
    """
    url = f"{POWERBI_API_BASE}/groups/{WORKSPACE_ID}/datasets/{DATASET_ID}/refreshes"
    headers = {
        "Authorization": f"Bearer {access_token}",
        "Content-Type": "application/json",
    }
    # Empty JSON body triggers a standard refresh; see MS docs for enhanced options.
    resp = requests.post(url, headers=headers, json={})
    resp.raise_for_status()
    # Depending on API behavior, this may be empty or contain minimal JSON.
    return resp.json() if resp.content else {}


def get_refresh_history(access_token, top=1):
    """
    Get the latest refresh history entries for the dataset.
    """
    url = (
        f"{POWERBI_API_BASE}/groups/{WORKSPACE_ID}/datasets/"
        f"{DATASET_ID}/refreshes?$top={top}"
    )
    headers = {"Authorization": f"Bearer {access_token}"}
    resp = requests.get(url, headers=headers)
    resp.raise_for_status()
    return resp.json().get("value", [])


def main():
    token = get_access_token()
    print("Triggering dataset refresh...")
    trigger_response = trigger_refresh(token)
    print("Refresh trigger response:", trigger_response)

    # Poll refresh history to watch status.
    # Power BI REST API returns statuses such as Unknown, Completed, Failed, Disabled.
    # Poll every 10 seconds for up to ~2 minutes.
    print("Polling refresh status...")
    for i in range(12):
        history = get_refresh_history(token, top=1)
        if not history:
            print(f"[{i+1}] No refresh records yet.")
        else:
            latest = history[0]
            status = latest.get("status", "Unknown")
            start_time = latest.get("startTime")
            end_time = latest.get("endTime")
            print(
                f"[{i+1}] Status: {status}, "
                f"Start: {start_time}, End: {end_time}"
            )
            if status.lower() in ("completed", "failed", "disabled", "unknown"):
                print("Final status reached:", status)
                break
        time.sleep(10)
    else:
        print("Polling finished without reaching a final status. Check Power BI service UI.")


if __name__ == "__main__":
    main()

"""a) MySQL stored procedure — sql/sp_merge_staging.sql

File: sql/sp_merge_staging.sql
"""

-- Create main table (example)
CREATE TABLE IF NOT EXISTS daily_sales (
  order_id VARCHAR(64) NOT NULL,
  date DATE,
  product VARCHAR(255),
  quantity INT,
  price DECIMAL(12,2),
  revenue DECIMAL(14,2),
  updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP,
  PRIMARY KEY (order_id)
);

-- Create staging table
CREATE TABLE IF NOT EXISTS daily_sales_staging LIKE daily_sales;

-- Stored procedure to merge staging -> main
DROP PROCEDURE IF EXISTS sp_merge_staging_to_main;
DELIMITER $$
CREATE PROCEDURE sp_merge_staging_to_main()
BEGIN
  -- Upsert using INSERT ... ON DUPLICATE KEY UPDATE
  INSERT INTO daily_sales (order_id, date, product, quantity, price, revenue)
  SELECT order_id, date, product, quantity, price, revenue FROM daily_sales_staging
  ON DUPLICATE KEY UPDATE
    date = VALUES(date),
    product = VALUES(product),
    quantity = VALUES(quantity),
    price = VALUES(price),
    revenue = VALUES(revenue),
    updated_at = CURRENT_TIMESTAMP;

  -- Clear staging after successful merge
  TRUNCATE TABLE daily_sales_staging;
END $$
DELIMITER ;

"""b) Python loader — scripts/sql_incremental_load.py

File: scripts/sql_incremental_load.py
"""

#!/usr/bin/env python3
"""
sql_incremental_load.py
Writes a cleaned CSV to the staging table and calls stored procedure to merge into main.

Usage:
    python scripts/sql_incremental_load.py --in data/clean/sales_clean.csv

Requires env:
- DB_USER, DB_PASS, DB_HOST, DB_PORT, DB_NAME
"""
import os
import argparse
import pandas as pd
from sqlalchemy import create_engine, text
from dotenv import load_dotenv

load_dotenv()

def get_engine():
    user = os.getenv('DB_USER', 'root')
    pwd = os.getenv('DB_PASS', '')
    host = os.getenv('DB_HOST', 'localhost')
    port = os.getenv('DB_PORT', '3306')
    db = os.getenv('DB_NAME', 'salesdb')
    uri = f"mysql+pymysql://{user}:{pwd}@{host}:{port}/{db}?charset=utf8mb4"
    engine = create_engine(uri, pool_pre_ping=True)
    return engine

def load_to_staging(csv_path, engine, staging_table="daily_sales_staging"):
    df = pd.read_csv(csv_path)
    # Ensure order_id present - adapt to your dataset key
    if 'order_id' not in df.columns:
        raise SystemExit("CSV must contain 'order_id' column to perform upsert.")
    # Optional: convert dtypes
    df.to_sql(staging_table, engine, if_exists='replace', index=False)
    print(f"Wrote {len(df)} rows to staging table '{staging_table}'")

def call_merge_proc(engine):
    with engine.begin() as conn:
        conn.execute(text("CALL sp_merge_staging_to_main();"))
    print("Called sp_merge_staging_to_main successfully")

def main():
    p = argparse.ArgumentParser()
    p.add_argument('--in', dest='infile', required=True)
    args = p.parse_args()
    eng = get_engine()
    load_to_staging(args.infile, eng)
    call_merge_proc(eng)

if __name__ == "__main__":
    main()

"""3) Price Alert Automation — scraper + history + alerts

Overview:

Scrape product pages (generic approach; works if the page structure is known).

Store price history in SQLite.

If price <= threshold or price has dropped significantly, send an alert via Email and Telegram.

Files:

scripts/price_scraper.py (main script)

scripts/utils_alerts.py (helpers to send email/telegram)
"""

#!/usr/bin/env python3
"""
utils_alerts.py
Helper functions to send email and Telegram alerts.
Requires env vars:
- SMTP_HOST, SMTP_PORT, SMTP_USER, SMTP_PASS
- TELEGRAM_BOT_TOKEN, TELEGRAM_CHAT_ID
"""
import os
import smtplib
from email.message import EmailMessage
import requests

def send_email(subject, body, to_addrs):
    host = os.getenv('SMTP_HOST', 'smtp.gmail.com')
    port = int(os.getenv('SMTP_PORT', 587))
    user = os.getenv('SMTP_USER')
    pwd = os.getenv('SMTP_PASS')
    if not (user and pwd):
        raise RuntimeError("SMTP credentials not set")
    msg = EmailMessage()
    msg['Subject'] = subject
    msg['From'] = user
    msg['To'] = to_addrs if isinstance(to_addrs, str) else ', '.join(to_addrs)
    msg.set_content(body)
    with smtplib.SMTP(host, port) as s:
        s.starttls()
        s.login(user, pwd)
        s.send_message(msg)
    print("Email alert sent to", msg['To'])

def send_telegram(message):
    token = os.getenv('TELEGRAM_BOT_TOKEN')
    chat_id = os.getenv('TELEGRAM_CHAT_ID')
    if not token or not chat_id:
        print("Telegram credentials not configured.")
        return
    url = f"https://api.telegram.org/bot{token}/sendMessage"
    resp = requests.post(url, json={"chat_id": chat_id, "text": message})
    resp.raise_for_status()
    print("Telegram alert sent")

#!/usr/bin/env python3
"""
price_scraper.py
Simple price scraper that stores history in SQLite and sends alerts.

Usage:
    python scripts/price_scraper.py --url "<product_page>" --name "Product Name" --threshold 15000

Notes:
- This is a generic scraper. You must adapt the `extract_price` function per site.
- For Amazon/Flipkart, HTML is dynamic and complex; prefer official APIs or a headless browser for reliability.
"""
import argparse
import sqlite3
import time
import re
import requests
from bs4 import BeautifulSoup
from datetime import datetime
from utils_alerts import send_email, send_telegram
import os

DB_PATH = os.getenv("PRICE_DB_PATH", "data/price_history.db")
os.makedirs(os.path.dirname(DB_PATH), exist_ok=True)

def fetch_html(url):
    headers = {
        "User-Agent": "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/115.0 Safari/537.36"
    }
    r = requests.get(url, headers=headers, timeout=15)
    r.raise_for_status()
    return r.text

def extract_price(html, url):
    """
    Naive price extractor:
    - Look for common patterns of currency and numbers.
    - For robust scraping, inspect target page and write CSS selectors.
    """
    soup = BeautifulSoup(html, "html.parser")
    text = soup.get_text(separator=" ", strip=True)
    # Common currency symbols
    # Search for patterns like ₹ 12,345 or Rs. 12,345 or 12,345.00
    m = re.search(r'₹\s*[\d,]+(?:\.\d{1,2})?', text)
    if not m:
        m = re.search(r'Rs\.?\s*[\d,]+(?:\.\d{1,2})?', text)
    if not m:
        # fallback: first occurrence of number with commas
        m = re.search(r'[\d,]{2,}\.\d{1,2}|\d{1,3}(?:,\d{3})+(?:\.\d{1,2})?', text)
    if not m:
        raise ValueError("Price not found - adapt extract_price for this site")
    price_text = m.group(0)
    # Remove non-numeric characters
    num = re.sub(r'[^\d.]', '', price_text)
    return float(num)

def init_db(conn):
    conn.execute("""
    CREATE TABLE IF NOT EXISTS price_history (
      id INTEGER PRIMARY KEY AUTOINCREMENT,
      product_name TEXT,
      url TEXT,
      price REAL,
      checked_at TEXT
    )
    """)
    conn.commit()

def insert_price(conn, product_name, url, price):
    now = datetime.utcnow().isoformat()
    conn.execute("INSERT INTO price_history (product_name, url, price, checked_at) VALUES (?, ?, ?, ?)",
                 (product_name, url, price, now))
    conn.commit()

def get_last_price(conn, product_name):
    cur = conn.execute("SELECT price, checked_at FROM price_history WHERE product_name = ? ORDER BY id DESC LIMIT 1", (product_name,))
    row = cur.fetchone()
    return (row[0], row[1]) if row else (None, None)

def run_check(url, product_name, threshold_price=None, drop_percent=5.0, recipients=None):
    html = fetch_html(url)
    price = extract_price(html, url)
    print(f"Current price for {product_name}: {price}")
    conn = sqlite3.connect(DB_PATH)
    init_db(conn)
    last_price, last_time = get_last_price(conn, product_name)
    insert_price(conn, product_name, url, price)
    alert = False
    reasons = []
    if threshold_price is not None and price <= threshold_price:
        alert = True
        reasons.append(f"Price <= threshold ({price} <= {threshold_price})")
    if last_price:
        drop = ((last_price - price) / last_price) * 100
        if drop >= drop_percent:
            alert = True
            reasons.append(f"Price dropped {drop:.2f}% from last {last_price} to {price}")
    if alert:
        body = f"Price alert for {product_name}\nURL: {url}\nPrice: {price}\nReasons:\n" + "\n".join(reasons)
        print("Sending alert:", body)
        # Send email (if recipients provided) and Telegram (if configured)
        if recipients:
            send_email(f"Price Alert: {product_name}", body, recipients)
        send_telegram(body)
    else:
        print("No alert triggered.")
    conn.close()

if __name__ == "__main__":
    p = argparse.ArgumentParser()
    p.add_argument('--url', required=True)
    p.add_argument('--name', required=True)
    p.add_argument('--threshold', type=float, default=None)
    p.add_argument('--drop', type=float, default=5.0, help="percentage drop that triggers alert")
    p.add_argument('--to', dest='recipients', default=None, help="comma-separated emails")
    args = p.parse_args()
    recips = [x.strip() for x in args.recipients.split(',')] if args.recipients else None
    run_check(args.url, args.name, threshold_price=args.threshold, drop_percent=args.drop, recipients=recips)

"""Environment variables (summary)

Add these to .env or GitHub secrets:
"""

# Power BI
TENANT_ID=
CLIENT_ID=
CLIENT_SECRET=
WORKSPACE_ID=
DATASET_ID=

# Database
DB_USER=
DB_PASS=
DB_HOST=
DB_PORT=3306
DB_NAME=

# SMTP for email alerts
SMTP_HOST=smtp.gmail.com
SMTP_PORT=587
SMTP_USER=
SMTP_PASS=

# Telegram
TELEGRAM_BOT_TOKEN=
TELEGRAM_CHAT_ID=

# Price scraper
PRICE_DB_PATH=data/price_history.db